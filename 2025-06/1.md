# RAG初探

## 1. Cognee项目 https://github.com/topoteretes/cognee

### 1.1 封装好的整体pipeline

将传统 RAG 的向量检索与知识图谱相结合，并通过本体论进行约束和增强。
- **知识图谱是主要方法**
- **在有本体(Ontologies)的情况下效果会好很多**

### 1.2 search https://docs.cognee.ai/reference/search-types
支持多种搜索的参数和设置:  而且支持混合搜索
```python
from cognee.modules.search.types import SearchType
 
# Available search types
SearchType.SUMMARIES 
SearchType.INSIGHTS 
SearchType.CHUNKS 
SearchType.COMPLETION
SearchType.GRAPH_COMPLETION
SearchType.GRAPH_SUMMARY_COMPLETION
SearchType.GRAPH_COMPLETION_COT
SearchType.GRAPH_COMPLETION_CONTEXT_EXTENSION
SearchType.CODE
```
![image](https://github.com/user-attachments/assets/8903d076-63e3-47bb-90e0-5650f103fc95)

### 1.3本体

本体论本质上是一种表示知识的正式框架。虽然听起来可能有些抽象，但在实践中它非常强大。与基本的图结构不同，本体论提供：

- RDF frameworks - Standardized graph models for representing data RDF 框架 - 用于表示数据的标准化图模型
- Class inheritance - Hierarchical relationships (like how a Tesla is a Car, which is a Vehicle) 类继承 - 分层关系（例如 Tesla 是汽车，汽车是交通工具）
- Transitive reasoning - If A→B and B→C, then A→C (if Toyota makes Lexus, and Lexus makes the ES model, then Toyota makes the ES model) 传递推理 - 如果 A→B 且 B→C，则 A→C（如果丰田制造雷克萨斯，且雷克萨斯制造 ES 车型，则丰田制造 ES 车型）
- Symmetric properties - Two-way relationships (if Alice is Bob's sibling, Bob is Alice's sibling) 对称属性 - 双向关系（如果 Alice 是 Bob 的兄弟姐妹，那么 Bob 也是 Alice 的兄弟姐妹）

这些功能看似微不足道，但在你试图从复杂数据集中提取有意义的洞察时，它们却有着天壤之别。

```rdf
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:ns1="http://example.org/ontology#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
>
  <rdf:Description rdf:about="http://example.org/ontology#Volkswagen">
    <rdfs:comment>Created for making cars accessible to everyone.</rdfs:comment>
    <ns1:produces rdf:resource="http://example.org/ontology#VW_Golf"/>
    <ns1:produces rdf:resource="http://example.org/ontology#VW_ID4"/>
    <ns1:produces rdf:resource="http://example.org/ontology#VW_Touareg"/>
    <rdf:type rdf:resource="http://example.org/ontology#CarManufacturer"/>
    <rdf:type rdf:resource="http://www.w3.org/2002/07/owl#NamedIndividual"/>
  </rdf:Description>
  <!-- Additional ontology definitions follow -->
</rdf:RDF>
```

**结果对比**
![image](https://github.com/user-attachments/assets/a3b2d084-cb50-44ed-ac6c-6c6e58ea113e)


## 2. 通义千问 QWen-Agent https://github.com/QwenLM/Qwen-Agent

### 2.1 配置参数

![image](https://github.com/user-attachments/assets/6adcdb67-46d2-44a9-b733-c49b3825b212)


### 2.2 搜索工具 (qwen_agent/tools/search_tools)

- keyword_search(bm25) - 基于BM25算法的关键词搜索
- vector_search(embedding,使用langchain) - 基于向量嵌入的语义搜索
- hybrid_search - 混合搜索策略



### 2.3 关键词生成策略 (qwen_agent/agents/keygen_strategies)

- 普通方法 - 直接关键词提取
- with_knowledge方法 - 结合知识库的关键词生成
- 先split方法 - 分割后生成关键词
>注意：用**大模型**实现所有的关键词生成


### 2.4 检索整合 (qwen_agent/tools/retrieval.py)


### 2.5 记忆机制 (qwen_agent/memory/memory.py)

- memory的实现：调用LLM让其自主存储重要内容
- 在下一次对话时作为system prompt返回
- 实现持续学习和上下文记忆功能

## 3.E²GraphRAG - https://github.com/YiboZhao624/E-2GraphRAG

> TL;DR:针对长篇文章的rag方案，同时考虑速度与准确度，采用摘要树（每k个chunk组合成一个父节点）+知识图谱（spacy提取命名实体，边为实体在同一句子或段落中的共现关系，权重为共现次数。）的混合检索方式进行rag。

### 1.核心逻辑

E²GraphRAG 的核心思想是"先解构，后重组，再问答"。它通过将长文档预处理成两种互补的结构化数据——**摘要树**和**知识图谱**，从而实现高效且精准的检索增强生成（RAG）。

### 2.核心流程

#### Step 1：初始化与准备 (Setup & Preparation)

**配置加载**

调用 `parse_args()` 函数来解析和加载系统配置参数。

**数据加载**

调用 `utils.load_dataset(configs)` 来加载待处理的文档数据集。

**分词器加载**

通过 `tokenizer = AutoTokenizer.from_pretrained(...)` 加载预训练的分词器模型。

#### Step 2：主循环 - 逐篇文档处理 (The Main Loop: Per-Document Processing)

程序进入处理单篇长文档的主要生命周期，通过 `for i, data_piece in enumerate(dataset):` 遍历每个文档。

**文本切分**

首先获取文档内容 `text = data_piece["book"]`，然后调用 `utils.sequential_split(text, tokenizer, ...)` 函数。这一步将整篇长文档切分成一系列有重叠的、固定长度的文本块（chunks），这些文本块构成了所有后续结构化处理的基础单元。

**并行预处理（核心环节）**

调用 `parallel_build_extract(text, configs, ...)` 函数，这是整个系统最关键的部分。主进程创建一个包含两个worker的进程池，同时派发两个独立的异步任务：

- **任务A（GPU密集型）**：执行 `build_tree_task`，目标是构建摘要树结构
- **任务B（CPU密集型）**：执行 `extract_graph_task`，目标是提取知识图谱

这两个任务在独立的子进程中执行：

- **子进程A**：在隔离环境中加载大语言模型（LLM）到GPU，调用 `build_tree.py` 中的核心逻辑，对文本块进行层层总结，最终生成 `tree.json` 文件
- **子进程B**：在隔离环境中加载spaCy模型到CPU，调用 `extract_graph.py` 中的核心逻辑，提取实体和关系，最终生成 `graph.json`、`index.json` 等文件

主进程派发任务后会暂停等待，通过 `build_future.get()` 和 `extract_future.get()` 收集两个子进程的工作成果和耗时信息。

**问答模型加载**

通过 `llm = AutoModelForCausalLM.from_pretrained(...)` 或 `pipeline(...)` 加载用于问答的语言模型。

**检索器初始化（智能核心）**

创建 `retriever = Retriever(tree, graph, ...)` 对象，这是系统的智能核心部分：

- **接收数据**：将刚构建好的树、图、实体索引等所有结构化数据传入检索器
- **构建向量索引**：在初始化过程中，检索器调用 `_build_faiss_index` 方法，遍历树中的所有节点（包括原文和各级摘要），用SentenceTransformer模型将它们编码成向量，构建FAISS索引，这是实现快速全局语义搜索的关键基础设施

**问答子循环 - 逐个问题回答（The Inner Loop: Per-Question Answering）**

通过 `for j, qa_piece in enumerate(qa):` 开始回答与当前文档相关的每个问题：

**智能检索**

调用 `model_supplement = retriever.query(question, ...)` 执行混合检索策略，这是整个框架最精妙的部分：

- **第一步：实体提取** - 从问题中识别并提取关键实体
- **第二步：本地优先** - 尝试在知识图谱中找到实体间的紧密联系，定位到最相关的原文块，这是高精度的结构化搜索
- **第三步：全局回退** - 如果本地搜索失败，启动备用方案：使用FAISS向量索引进行全局语义搜索，找到与问题最相似的文本块（可能是原文或摘要），并结合实体信息进行重排序

**答案生成**

组合Prompt模板，然后调用 `llm(...)` 生成答案。

**结果记录**

通过 `res.append({...})` 将问题、标准答案、模型输出、引用证据等所有相关信息打包成字典，添加到结果列表中。

**保存与清理**

使用 `json.dump(res, ...)` 保存结果，然后通过 `del llm` 和 `torch.cuda.empty_cache()` 进行资源清理。当一篇文档的所有问题都处理完毕后，将结果列表保存为JSON文件，并显式删除LLM对象清空GPU缓存，这对于释放显存、确保下一篇文档有足够资源处理至关重要。

### 3.核心模块详解


#### a.摘要树构建 (`build_tree.py`)

摘要树构建采用每k个子节点生成一个父节点的方式，相当于对知识进行浓缩处理。

**目标**：创建一个从具体到抽象的多层次内容索引结构。

**方法**：叶子节点保持为原始文本块的形式，而父节点则使用大语言模型（LLM）对下层的多个节点（原文或低层摘要）进行有损但高效的内容总结。

**产出**：生成 `tree.json` 文件，这是一个完整记录了所有节点文本内容和父子关系的树形结构。

#### b.知识图谱提取 (`extract_graph.py`)

**目标**：捕捉并构建文档中实体间的关系网络。

**方法**： 节点（Nodes）通过spaCy识别文档中的实体，包括人名、地名、核心名词等重要概念。边（Edges）表示实体在同一句子或段落中的共现关系，权重由共现次数决定。

**产出**：生成多个文件，包括 `graph.json`（图结构）、`index.json`（实体到文本块的倒排索引）、`appearance_count.json`（实体频率统计）。

#### c.智能检索器 (`query.py`)

(1)**实体提取** 从输入问题中识别并提取关键实体信息。

(2)**本地优先策略（Local-First）** 在知识图谱中查找实体间的最短路径，定位到同时包含这些强相关实体的文本块。当结果过多时，系统会自动收紧图搜索的距离限制（`shortest_path_k`），进行迭代筛选以获得更精准的结果。

(3)**全局回退策略（Global-Fallback）** 当本地搜索无果或问题中不包含实体时触发：

首先使用FAISS索引进行全局语义搜索，召回一批候选文本块（包含各级摘要）。然后进行智能重排（Re-ranking），根据候选块包含的问题实体数量和频率进行重排序，选出最优质的证据。


### 4.针对Kaggle的优化思路

**核心想法**：保留分层结构的优势，但用无损索引替换有损摘要，提高信息保真度。

**叶子节点**：保持不变，继续使用原文文本块作为基础单元。

**父节点（元数据节点）**：不再使用LLM生成的摘要，而是采用其所有子节点文本块的元数据聚合方式：

- **关键词/实体索引**：聚合子节点中出现的所有Kaggle相关的通用思想或方法等关键信息
- **假设性问答索引**：预先用LLM对每个子节点生成问答对，父节点存储这些结构化的问答信息，提高检索的针对性和准确性



