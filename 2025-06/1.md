# 每周任务清单 (2025.06.09 - 2025.06.15)

## 1. Agent内部记忆机制实现

- [ ] **研究Agent内部记忆实现方案**
  - 探索 [cognee项目](https://github.com/topoteretes/cognee?tab=readme-ov-file) 框架
  - 评估对RAG系统的潜在改进作用
  - 如有需要可联系相关媒体获取更多信息

## 2. 通义千问 QWen-Agent

### 2.1 配置参数

![image](https://github.com/user-attachments/assets/6adcdb67-46d2-44a9-b733-c49b3825b212)


### 2.2 搜索工具 (qwen_agent/tools/search_tools)

- keyword_search(bm25) - 基于BM25算法的关键词搜索
- vector_search(embedding,使用langchain) - 基于向量嵌入的语义搜索
- hybrid_search - 混合搜索策略



### 2.3 关键词生成策略 (qwen_agent/agents/keygen_strategies)

- 普通方法 - 直接关键词提取
- with_knowledge方法 - 结合知识库的关键词生成
- 先split方法 - 分割后生成关键词
>注意：用**大模型**实现所有的关键词生成


### 2.4 检索整合 (qwen_agent/tools/retrieval.py)


### 2.5 记忆机制 (qwen_agent/memory/memory.py)

- memory的实现：调用LLM让其自主存储重要内容
- 在下一次对话时作为system prompt返回
- 实现持续学习和上下文记忆功能

## 3. E²GraphRAG - 新型RAG方法


### 1. 核心逻辑与流程

E²GraphRAG 的核心思想是“先解构，后重组，再问答”。它通过将长文档预处理成两种互补的结构化数据——**摘要树**和**知识图谱**，从而实现高效且精准的检索增强生成（RAG）。

#### **伪代码表示其核心流程:**


**场景一: 初始化与准备 (Setup & Preparation)**

1.  **[配置加载]**:
    *   **动作**: 调用 `parse_args()`。

2.  **[数据加载]**:
    *   **动作**: 调用 `utils.load_dataset(configs)`。

3.  **[分词器加载]**:
    *   **动作**: `tokenizer = AutoTokenizer.from_pretrained(...)`。

---

**场景二: 主循环 - 逐篇文档处理 (The Main Loop: Per-Document Processing)**

*   **动作**: `for i, data_piece in enumerate(dataset):`
*   **循环开始**: 程序进入处理单篇长文档的生命周期。

    1.  **[文本切分]**:
        *   **动作**: `text = data_piece["book"]`，然后调用 `utils.sequential_split(text, tokenizer, ...)`。
        *   **背后逻辑**: 将一整篇长长的文档，切分成一系列有重叠的、固定长度的文本块（chunks）。这是所有后续结构化处理的基础单元。

    2.  **[并行预处理] - (核心)**:
        *   **动作**: 调用 `parallel_build_extract(text, configs, ...)`。
        *   **任务分派**: 主进程创建了一个包含两个worker的**进程池**。它**同时**向这个池子派发两个独立的、异步的任务：
          
                -   **任务A (GPU密集型)**: 执行 `build_tree_task`。这个任务的目标是构建**摘要树**。
                -   **任务B (CPU密集型)**: 执行 `extract_graph_task`。这个任务的目标是提取**知识图谱**。
        *  **独立执行**:
          
                *   **子进程A**: 在自己的隔离环境中，加载**大语言模型(LLM)到GPU**，然后调用 `build_tree.py` 中的核心逻辑，对文本块进行层层总结，最终生成 `tree.json`。
                *   **子进程B**: 在自己的隔离环境中，加载**spaCy模型到CPU**，然后调用 `extract_graph.py` 中的核心逻辑，提取实体和关系，最终生成 `graph.json`, `index.json` 等。
        *  **结果等待与收集**: 主进程在派发完任务后，会暂停并等待 `build_future.get()` 和 `extract_future.get()`。它会一直等到两个子进程都完成工作，并把它们的成果（树和图的数据）和耗时信息收集回来。

    3.  **[问答模型加载]**:
        *   **动作**: `llm = AutoModelForCausalLM.from_pretrained(...)` 或 `pipeline(...)`。

    4.  **[检索器初始化] - (智能核心)**:
        *   **动作**: `retriever = Retriever(tree, graph, ...)`。
        *   **背后逻辑**:
            a. **接收数据**: 创建一个 `Retriever` 对象，将刚刚构建好的**树、图、实体索引**等所有结构化数据都“喂”给它。
            b. **构建向量索引**: 在初始化过程中，`Retriever` 会调用 `_build_faiss_index`。它会遍历树中的**所有节点**（包括原文和各级摘要），用一个 `SentenceTransformer` 模型将它们全部编码成向量，并构建一个**FAISS索引**。这个索引是实现快速全局语义搜索的关键。

    5.  **[问答子循环] - 逐个问题回答 (The Inner Loop: Per-Question Answering)**:
        *   **动作**: `for j, qa_piece in enumerate(qa):`
        *   **循环开始**: 程序开始回答与当前文档相关的每一个问题。

            a. **[智能检索]**:
                *   **动作**: `model_supplement = retriever.query(question, ...)`。
                *   **背后逻辑**: 这是整个框架最精妙的部分。`query` 方法会执行一套**混合检索策略**:
                    *   **第一步: 实体提取** - 从问题中找出关键实体。
                    *   **第二步: 本地优先** - 尝试在**知识图谱**中找到实体间的紧密联系，从而定位到最相关的原文块。这是一个高精度的结构化搜索。
                    *   **第三步: 全局回退** - 如果本地搜索失败，则启动备用方案：使用**FAISS向量索引**进行全局语义搜索，找到与问题最相似的文本块（可能是原文，也可能是摘要），并结合实体信息进行重排序。

            b. **[答案生成]**:
                *   **动作**: 组合Prompt，然后 `llm(...)`。
                *   **背后逻辑**:
                    *   从 `prompt_dict.py` 中取出预设的模板。
                    *   将**问题**和上一步检索到的**证据 (`model_supplement`)** 填入模板，形成最终的输入文本。
                    *   将这个输入送入已加载的LLM，得到模型的输出（对于选择题是各选项的概率，对于开放题是生成的文本）。

            c. **[结果记录]**:
                *   **动作**: `res.append({...})`。
                *   **背后逻辑**: 将问题、标准答案、模型输出、引用的证据等所有相关信息打包成一个字典，添加到一个名为 `res` 的列表中。

    6.  **[保存与清理]**:
        *   **动作**: `json.dump(res, ...)`，然后 `del llm` 和 `torch.cuda.empty_cache()`。
        *   **背后逻辑**:
            *   当一篇文档的所有问题都回答完毕后，将 `res` 列表保存为JSON文件。
            *   **至关重要**: 显式地删除LLM对象并清空GPU缓存。这释放了大量显存，确保在处理下一篇长文档时，系统有足够的资源，避免内存溢出。


### 2. 核心模块详解

#### 2.1 预处理双引擎 (Preprocessing Engines)

- **摘要树构建 (`build_tree.py`)**: 每k个子节点生成一个父节点，相当于浓缩知识
    - **目标**: 创建一个从具体到抽象的多层次内容索引。
    - **方法**:
        1.  **叶子节点**: 原始文本块。
        2.  **父节点**: 使用**大语言模型(LLM)**对下层多个节点（原文或低层摘要）进行有损但高效的**内容总结**。
    - **产出**: `tree.json`，一个记录了所有节点文本、父子关系的树形结构。
      
- **知识图谱提取 (`extract_graph.py`)**:
    - **目标**: 捕捉文档中实体间的关系网络。
    - **方法**:
        1.  **节点 (Nodes)**: 使用`spaCy`识别出的实体（人名、地名、核心名词等）。
        2.  **边 (Edges)**: 实体在同一句子或段落中的**共现关系**，权重为共现次数。
    - **产出**: `graph.json` (图结构), `index.json` (实体->文本块的倒排索引), `appearance_count.json` (实体频率统计)。

#### 2.2 智能检索器 (`query.py`)

- **策略核心**:
    1.  **实体提取**: 从问题中识别出关键实体。
    2.  **本地优先 (Local-First)**:
        - **方法**: 在**知识图谱**中查找实体间的最短路径，定位到同时包含这些强相关实体的文本块。
        - **动态调整**: 如果结果过多，则**自动收紧**图搜索的距离限制 (`shortest_path_k`)，进行迭代筛选。
    3.  **全局回退 (Global-Fallback)**:
        - **触发时机**: 本地搜索无果，或问题中无实体。
        - **方法**:
            a. **向量召回**: 使用**FAISS索引**进行全局语义搜索，召回一批候选文本块（包含各级摘要）。
            b. **智能重排 (Re-ranking)**: 对召回的候选块，根据其包含的**问题实体数量和频率**进行重排序，选出最优质的证据。
- **重要特点**:
    - **多粒度证据**: 检索出的证据可以是底层的原文片段，也可以是高层的摘要，为LLM提供最合适粒度的上下文。
    - **效率与效果的平衡**: 优先使用精准但范围小的图搜索，失败后才启动覆盖面广但可能稍粗糙的向量搜索，实现了资源的高效利用。

### 3. 针对Kaggle

**想法**: 保留分层结构，但用无损索引替换有损摘要。

- **叶子节点**: 保持不变，为原文文本块。
- **父节点 (元数据节点)**: 不再是LLM生成的摘要，而是其所有子节点文本块的**元数据聚合**。
    - **关键词/实体索引**: 聚合子节点中出现的所有kaggle的general idea或者方法等。
    - **假设性问答索引**: 预先用LLM对每个子节点生成Q&A对，父节点存储这些问答。

## 4. AI自动化学术综述生成

- [ ] **探索AI学术综述自动化工具**
  - 研读 [自动化报道](https://mp.weixin.qq.com/s/GJtPAf0hI9rfksJy7AduLQ)
  - 评估工具在研究工作流中的集成价值
  - 测试在当前项目中的应用可行性


## 5. PandaWiKi
- [ ] **RAG知识库（GO语言版）**
  - 阅读 [项目](https://pandawiki.docs.baizhi.cloud/)
  - 看看能不能把go改成python版本
